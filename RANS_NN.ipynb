{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173e30ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pietro\\anaconda3\\envs\\rans\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import SimulationData\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c983cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [19:49<00:00,  2.38s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [21:40<00:00,  2.60s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [18:58<00:00,  2.28s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [20:30<00:00,  2.46s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [20:41<00:00,  2.48s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [21:28<00:00,  2.58s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [22:26<00:00,  2.69s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [21:26<00:00,  2.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [23:13<00:00,  2.79s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [25:58<00:00,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = SimulationData(stencilNum=500,samplePerStencil=100, fileNames=[\"aw=1.1_internal.csv\",\"aw=1.2_internal.csv\",\"aw=1.3_internal.csv\", \"aw=1.4_internal.csv\",\"aw=1.5_internal.csv\",\"aw=1.6_internal.csv\",\"aw=1.7_internal.csv\",\"aw=1.8_internal.csv\",\"aw=1.9_internal.csv\",\"aw=2.0_internal.csv\"], override=True)\n",
    "#dataset.normalizeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc45c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "            \n",
    "            super(MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size1  = hidden_size1\n",
    "            self.hidden_size2 = hidden_size2\n",
    "            self.output_size = output_size\n",
    "            self.linear1 = nn.Linear(self.input_size, self.hidden_size1)\n",
    "            #self.batchNorm1 = nn.BatchNorm2d(self.hidden_size1)\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.linear2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "            #self.batchNorm2 = nn.BatchNorm2d(self.hidden_size2)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.linear3 = nn.Linear(self.hidden_size2, self.output_size)\n",
    "            #self.batchNorm2 = nn.BatchNorm2d(self.hidden_size2)\n",
    "            #self.relu3 = nn.ReLU()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden1 = self.linear1(x)\n",
    "            relu1  =self.relu1(hidden1)\n",
    "            hidden2 = self.linear2(relu1)\n",
    "            relu2 = self.relu2(hidden2)\n",
    "            output = self.linear3(relu2)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a86208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedSum, self).__init__()\n",
    "        self.G_split_size = 5\n",
    "    \n",
    "    def forward(self, G, input_features):\n",
    "        #print(f'if: {input_features[0]}')\n",
    "        G_ = G[:, :self.G_split_size]\n",
    "        #print(G_.shape[0])\n",
    "        L = (torch.matmul(torch.transpose(G, 1,0), input_features) / G.shape[0])\n",
    "        L_ = (torch.matmul(torch.transpose(input_features, 1, 0), G_) / G.shape[0])\n",
    "        D = torch.matmul(L, L_)\n",
    "        #print(f'D: {D}')\n",
    "        #print()\n",
    "        return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfe74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainNet(nn.Module):\n",
    "    def __init__(self, input_size1, hidden_size1_1,hidden_size1_2, output_size1,\n",
    "                 input_size2, hidden_size2_1, hidden_size2_2, output_size2):\n",
    "        \n",
    "        super(MainNet, self).__init__()\n",
    "        self.invariant_features_indeces = [4,5,6,7]\n",
    "\n",
    "        self.embeddingNetwork = MLP(input_size1,hidden_size1_1, hidden_size1_2, output_size1)\n",
    "        self.weightedSum = WeightedSum()\n",
    "        self.flatten = nn.Flatten(1,-1)\n",
    "        self.fittingNetwork = MLP(input_size2,hidden_size2_1, hidden_size2_2, output_size2)\n",
    "        \n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "    \n",
    "        #print(f'input values: {x[0]}\\n shape: {x.shape}')\n",
    "        invariant_x = x[:, self.invariant_features_indeces]\n",
    "        \n",
    "        embedding_output = self.embeddingNetwork(invariant_x)\n",
    "        #print(f' shape: {embedding_output.shape}')\n",
    "\n",
    "        weighted_output = self.weightedSum(embedding_output, x)\n",
    "        #print(f' shape: {weighted_output.shape}')\n",
    "        \n",
    "        weighted_output_flattened = torch.flatten(weighted_output)\n",
    "        #print(f' shape: {weighted_output_flattened.shape}')\n",
    "        \n",
    "        fitting_output = self.fittingNetwork(weighted_output_flattened)\n",
    "        #print(f'fitting output: {fitting_output}\\n shape: {fitting_output.shape}')\n",
    "\n",
    "        return fitting_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd62a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size1 = 4\n",
    "hidden_size1_1 = 32\n",
    "hidden_size1_2 = 64\n",
    "output_size1 = 256\n",
    "\n",
    "input_size2 = 256*5\n",
    "hidden_size2_1 = 64\n",
    "hidden_size2_2 = 32\n",
    "output_size2 = 3\n",
    "\n",
    "net = MainNet(input_size1, hidden_size1_1, hidden_size1_2, output_size1, input_size2,hidden_size2_1, hidden_size2_2, output_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256fb836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4946"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b47efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "test_perc = 0.22\n",
    "train_perc = 1-test_perc\n",
    "train_length = math.ceil(len(dataset) * train_perc)\n",
    "test_length = math.ceil(len(dataset) *test_perc)\n",
    "train_length = train_length + (len(dataset)-train_length-test_length)\n",
    "train, test = torch.utils.data.random_split(dataset, [train_length,test_length], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46352b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(dataset= train, batch_size = 1, num_workers=1)\n",
    "loader_test = DataLoader(dataset= test, batch_size = 1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8375cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecefadd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef test():\\n    net.eval()\\n    with torch.no_grad():\\n        losses = []\\n        counter = 0\\n        for i, data in enumerate(loader_test):\\n            counter +=1\\n            x_batch,y_batch = data\\n            for i in range(len(x_batch)):\\n                x = x_batch[i]\\n                y = y_batch[i]\\n                output1, output2, output3 = net(x.float())\\n                loss1 = criterion(output1.reshape(1).float(), y[[0]].float())\\n                loss2 = criterion(output2.reshape(1).float(), y[[1]].float())\\n                loss3 = criterion(output3.reshape(1).float(), y[[2]].float())\\n                loss = ((loss1)+(loss2)+(loss3)/3)\\n                losses.append(loss.item())\\n    print(f'avg testing loss: {sum(losses)/len(losses)}')\\n    return losses\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def test():\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        counter = 0\n",
    "        for i, data in enumerate(loader_test):\n",
    "            counter +=1\n",
    "            x_batch,y_batch = data\n",
    "            for i in range(len(x_batch)):\n",
    "                x = x_batch[i]\n",
    "                y = y_batch[i]\n",
    "                output1, output2, output3 = net(x.float())\n",
    "                loss1 = criterion(output1.reshape(1).float(), y[[0]].float())\n",
    "                loss2 = criterion(output2.reshape(1).float(), y[[1]].float())\n",
    "                loss3 = criterion(output3.reshape(1).float(), y[[2]].float())\n",
    "                loss = ((loss1)+(loss2)+(loss3)/3)\n",
    "                losses.append(loss.item())\n",
    "    print(f'avg testing loss: {sum(losses)/len(losses)}')\n",
    "    return losses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08b016f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import rotate\n",
    "\n",
    "def test_rotation(x, model):\n",
    "    with torch.no_grad():\n",
    "        #print()\n",
    "        #print(f'input: {x[0]}')\n",
    "\n",
    "        output1, output2, output3 = model(x.float())\n",
    "        rotate_cloud(x)\n",
    "        output1_rotated, output2_rotated, output3_rotated = model(x.float())\n",
    "\n",
    "    #print(f'input rotated: {x[0]}')\n",
    "    '''\n",
    "    print(f'{output1} - {output1_rotated} # pass: {(output1 - output1_rotated) <=0.0001}')\n",
    "    print(f'{output2} - {output2_rotated} # pass: {(output2 - output2_rotated) <=0.0001}')\n",
    "    print(f'{output3} - {output3_rotated} # pass: {(output3 - output3_rotated) <=0.0001}')\n",
    "    '''\n",
    "def rotate_cloud(x):\n",
    "    for point in x:\n",
    "        point[[0,1]] = rotate(-90,point[[0,1]])\n",
    "        point[[2,3]] = rotate(-90,point[[2,3]])\n",
    "        point[[8,9]] = rotate(-90,point[[8,9]])\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "026eeca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    net.train()\n",
    "    losses = []\n",
    "    losses_k = []\n",
    "    losses_eps = []\n",
    "    losses_nut = []\n",
    "    \n",
    "    losses_eval = []\n",
    "    losses_k_eval = []\n",
    "    losses_eps_eval = []\n",
    "    losses_nut_eval = []\n",
    "    info = []\n",
    "    counter = 0\n",
    "\n",
    "    for i, data in enumerate(zip(loader_train, loader_test)):\n",
    "        counter +=1\n",
    "        loader_train_temp,loader_test_temp = data\n",
    "        x_batch, y_batch = loader_train_temp\n",
    "        x_batch_test, y_batch_test = loader_test_temp\n",
    "        info.append({})\n",
    "        for i in range(len(x_batch)):\n",
    "            #print(x_batch)\n",
    "            x = x_batch[i].float()\n",
    "            y = y_batch[i].float()\n",
    "            #print(x_batch.grad)\n",
    "            #print(y)\n",
    "            x_test = x_batch_test[i].float()\n",
    "            y_test = y_batch_test[i].float()\n",
    "            \n",
    "            #test_rotation(x, net)\n",
    "            #test_rotation(x_test, net)\n",
    "            \n",
    "            info[counter-1][\"x\"] = x\n",
    "            info[counter-1][\"y\"] = y\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output1, output2, output3 = net(x)\n",
    "            loss1 = criterion(output1.reshape(1), y[[0]])\n",
    "            loss2 = criterion(output2.reshape(1), y[[1]])\n",
    "            loss3 = criterion(output3.reshape(1), y[[2]])\n",
    "            loss = ((loss1)+(loss2)+(loss3))/3\n",
    "            \n",
    "            '''\n",
    "            print('-')\n",
    "            print(f'output:{output1.reshape(1).float()} - true: { y[[0]].float()}')\n",
    "            print(f'output:{output2.reshape(1).float()} - true: { y[[1]].float()}')\n",
    "            print(f'output:{output3.reshape(1).float()} - true: { y[[2]].float()}')\n",
    "            print('-')\n",
    "            print(loss)\n",
    "            '''\n",
    "            \n",
    "            info[counter-1][\"output\"] = [output1.item(),output2.item(),output3.item()]\n",
    "            info[counter-1][\"loss\"] = loss.item()\n",
    "            info[counter-1][\"loss epsilon\"] = loss1.item()\n",
    "            info[counter-1][\"loss k\"] = loss2.item()\n",
    "            info[counter-1][\"loss nut\"] = loss3.item()\n",
    "            #print(net.fittingNetwork.linear3.weight)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(net.fittingNetwork.linear3.weight)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output1, output2, output3 = net(x_test.float())\n",
    "                loss1_eval = criterion(output1.reshape(1).float(), y_test[[0]].float())\n",
    "                loss2_eval = criterion(output2.reshape(1).float(), y_test[[1]].float())\n",
    "                loss3_eval = criterion(output3.reshape(1).float(), y_test[[2]].float())\n",
    "                loss_eval = ((loss1_eval)+(loss2_eval)+(loss3_eval))/3\n",
    "\n",
    "            #print(x[0])\n",
    "            #print(output1.item(), output2.item(), output3.item())\n",
    "            #print(y[0].item(), y[1].item(), y[2].item())\n",
    "            #print(loss.item())\n",
    "            #print()\n",
    "            #if(loss.item()>=0.20):\n",
    "            losses.append(loss.item())\n",
    "            losses_eps.append(loss1.item())\n",
    "            losses_k.append(loss2.item())\n",
    "            losses_nut.append(loss3.item())\n",
    "            \n",
    "            losses_eval.append(loss_eval.item())\n",
    "            losses_eps_eval.append(loss1_eval.item())\n",
    "            losses_k_eval.append(loss2_eval.item())\n",
    "            losses_nut_eval.append(loss3_eval.item())\n",
    "\n",
    "    return losses,losses_k,losses_eps, losses_nut,losses_eval,losses_eps_eval,losses_k_eval,losses_nut_eval, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e534d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_losses_k = []\n",
    "train_losses_eps = []\n",
    "train_losses_nut = []\n",
    "\n",
    "test_losses = []\n",
    "test_losses_k = []\n",
    "test_losses_eps = []\n",
    "test_losses_nut= []\n",
    "\n",
    "infos_train = []\n",
    "infos_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c51601",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 1/200 [00:11<38:46, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "epoch 0:\n",
      "total training loss 0.11901866704607124 total eval loss 0.18285657235412547\n",
      "nut training loss 0.14515400652783828 nut eval loss 0.21282743057784353\n",
      "eps training loss 0.09857042520117924 eps eval loss 0.15051489845899316\n",
      "k training loss 0.11333156879281461 k eval loss 0.18522738510541198\n",
      "#############\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  1%|▊                                                                                 | 2/200 [00:25<43:14, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "epoch 1:\n",
      "total training loss 0.1247795376262083 total eval loss 0.1852594947844788\n",
      "nut training loss 0.15224610330548527 nut eval loss 0.21681452627767642\n",
      "eps training loss 0.10364852195974644 eps eval loss 0.1464150067174314\n",
      "k training loss 0.1184439871580947 k eval loss 0.1925489539366074\n",
      "#############\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  2%|█▏                                                                                | 3/200 [00:42<48:59, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "epoch 2:\n",
      "total training loss 0.13459334468199338 total eval loss 0.19937319821727598\n",
      "nut training loss 0.15744560933953802 nut eval loss 0.2277894773414009\n",
      "eps training loss 0.10341451908927785 eps eval loss 0.14636505950057882\n",
      "k training loss 0.1429199039373273 k eval loss 0.22396505723161647\n",
      "#############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    loss,loss_k, loss_eps, loss_nut,loss_eval,loss_eps_eval,loss_k_eval,loss_nut_eval, info = train_one_epoch()\n",
    "    \n",
    "    train_losses.extend(loss)\n",
    "    train_losses_nut.extend(loss_nut)\n",
    "    train_losses_eps.extend(loss_eps)\n",
    "    train_losses_k.extend(loss_k)\n",
    "    \n",
    "    test_losses.extend(loss_eval)\n",
    "    test_losses_nut.extend(loss_nut_eval)\n",
    "    test_losses_eps.extend(loss_eps_eval)\n",
    "    test_losses_k.extend(loss_k_eval)\n",
    "    \n",
    "    infos_train.extend(info)\n",
    "    \n",
    "    print('#############')\n",
    "    print(f'epoch {i}:')\n",
    "    print(f'total training loss {sum(loss) / len(loss)} total eval loss {sum(loss_eval) / len(loss_eval)}')\n",
    "    print(f'nut training loss {sum(loss_nut) / len(loss_nut)} nut eval loss {sum(loss_nut_eval) / len(loss_nut_eval)}')\n",
    "    print(f'eps training loss {sum(loss_eps) / len(loss_eps)} eps eval loss {sum(loss_eps_eval) / len(loss_eps_eval)}')\n",
    "    print(f'k training loss {sum(loss_k) / len(loss_k)} k eval loss {sum(loss_k_eval) / len(loss_k_eval)}')\n",
    "    print('#############')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb64eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageOut(list_nums, n):\n",
    "    new_list = []\n",
    "    temp_sum = 0\n",
    "    temp_length = 0\n",
    "    counter = 0\n",
    "    average = 0\n",
    "    temp_list = []\n",
    "    final_list = []\n",
    "    for i, num in enumerate(list_nums):\n",
    "        counter+=1\n",
    "        temp_list.append(num)\n",
    "        if counter == n:\n",
    "            avg = sum(temp_list) / len(temp_list)\n",
    "            final_list.append(avg)\n",
    "            temp_list = []\n",
    "            counter = 0\n",
    "    if len(temp_list) != 0:\n",
    "        avg = sum(temp_list) / len(temp_list)\n",
    "        final_list.append(avg)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfe54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"log\")\n",
    "smoothing_factor = 1089\n",
    "\n",
    "plt.plot(averageOut(train_losses, smoothing_factor), color=\"red\", label=\"training loss\")\n",
    "plt.plot(averageOut(test_losses, smoothing_factor), color=\"blue\", label=\"validation loss\")\n",
    "\n",
    "#ax2.plot(averageOut(train_losses_k, smoothing_factor), color=\"red\", label=\"training loss\")\n",
    "#ax2.plot(averageOut(test_losses_k, smoothing_factor), color=\"blue\", label=\"validation loss\")\n",
    "#ax3.plot(averageOut(train_losses_nut, smoothing_factor), color=\"red\", label=\"training loss\")\n",
    "#ax3.plot(averageOut(test_losses_nut, smoothing_factor), color=\"blue\", label=\"validation loss\")\n",
    "plt.title(\"validation & training loss (log scale)\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./images/loss_net_C_4.0=500_S=100_E=300.png\", dpi=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_losses)/1089.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./models/net_C_4.0=500_S=100_E=300.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"./models/net.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "287199f5fcea6f9a85665f99d67797111be3aee227282d66a776eae29d9b3874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
