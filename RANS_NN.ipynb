{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173e30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import SimulationData\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c983cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimulationData(stencilNum=1000,samplePerStencil=50)\n",
    "#dataset.normalizeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b84a8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9834, dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dataset.y[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc45c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size1  = hidden_size1\n",
    "            self.hidden_size2 = hidden_size2\n",
    "            self.output_size = output_size\n",
    "            self.linear1 = nn.Linear(self.input_size, self.hidden_size1)\n",
    "            #self.batchNorm1 = nn.BatchNorm2d(self.hidden_size1)\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.linear2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "            #self.batchNorm2 = nn.BatchNorm2d(self.hidden_size2)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.linear3 = nn.Linear(self.hidden_size2, self.output_size)\n",
    "            #self.batchNorm2 = nn.BatchNorm2d(self.hidden_size2)\n",
    "            #self.relu3 = nn.ReLU()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden1 = self.linear1(x)\n",
    "            relu1  =self.relu1(hidden1)\n",
    "            hidden2 = self.linear2(relu1)\n",
    "            relu2 = self.relu2(hidden2)\n",
    "            output = self.linear3(relu2)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a86208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedSum, self).__init__()\n",
    "    \n",
    "    def forward(self, G, input_features):\n",
    "        G_split_size = 4\n",
    "        G_ = G[:, :G_split_size]\n",
    "        L = torch.matmul(torch.transpose(G, 1,0), input_features)\n",
    "        L_ = torch.matmul(torch.transpose(input_features, 1, 0), G_)\n",
    "        D = torch.matmul(L, L_)\n",
    "        return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dfe74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainNet(nn.Module):\n",
    "    def __init__(self, input_size1, hidden_size1_1,hidden_size1_2, output_size1,\n",
    "                 input_size2, hidden_size2_1, hidden_size2_2, output_size2):\n",
    "        \n",
    "        super(MainNet, self).__init__()\n",
    "        self.embeddingNetwork = MLP(input_size1,hidden_size1_1, hidden_size1_2, output_size1)\n",
    "        self.weightedSum = WeightedSum()\n",
    "        self.flatten = nn.Flatten(1,-1)\n",
    "        self.fittingNetwork = MLP(input_size2,hidden_size2_1, hidden_size2_2, output_size2)\n",
    "        \n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        norm_data = torch.nn.functional.normalize(x)\n",
    "        print(f'input values: {x}\\n shape: {x.shape}')\n",
    "        embedding_output = self.embeddingNetwork(norm_data)\n",
    "        print(f'embedding output: {embedding_output}\\n shape: {embedding_output.shape}')\n",
    "\n",
    "        #weighted_output = self.weightedSum(embedding_output, x)\n",
    "        print(f'weighted output: {weighted_output}\\n shape: {weighted_output.shape}')\n",
    "        \n",
    "        weighted_output_flattened = torch.flatten(embedding_output)\n",
    "        #print(f'weighted flattened: {weighted_output_flattened}\\n shape: {weighted_output_flattened.shape}')\n",
    "        \n",
    "        fitting_output = self.fittingNetwork(weighted_output_flattened)\n",
    "        #print(f'fitting output: {fitting_output}\\n shape: {fitting_output.shape}')\n",
    "\n",
    "        return fitting_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd62a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size1 = dataset.x.shape[2]\n",
    "hidden_size1_1 = 32\n",
    "hidden_size1_2 = 64\n",
    "output_size1 = 256\n",
    "\n",
    "input_size2 = 256*50\n",
    "hidden_size2_1 = 64\n",
    "hidden_size2_2 = 32\n",
    "output_size2 = 3\n",
    "\n",
    "net = MainNet(input_size1, hidden_size1_1, hidden_size1_2, output_size1, input_size2,hidden_size2_1, hidden_size2_2, output_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b47efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = torch.utils.data.random_split(dataset, [len(dataset)-100,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c97978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46352b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(dataset= dataset, batch_size = 1, num_workers=1)\n",
    "loader_test = DataLoader(dataset= dataset, batch_size = 1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be8375cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecefadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        counter = 0\n",
    "        for i, data in enumerate(loader_test):\n",
    "            counter +=1\n",
    "            x_batch,y_batch = data\n",
    "            for i in range(len(x_batch)):\n",
    "                x = x_batch[i]\n",
    "                y = y_batch[i]\n",
    "                output1, output2, output3 = net(x.float())\n",
    "                loss1 = criterion(output1.reshape(1).float(), y[[0]].float())\n",
    "                loss2 = criterion(output2.reshape(1).float(), y[[1]].float())\n",
    "                loss3 = criterion(output3.reshape(1).float(), y[[2]].float())\n",
    "                loss = (loss1*10)+(loss2*10)+(loss3*10)\n",
    "                losses.append(loss.item())\n",
    "    print(f'avg testing loss: {sum(losses)/len(losses)}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "026eeca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    net.train()\n",
    "    losses = []\n",
    "    counter = 0\n",
    "    eval_loss = []\n",
    "    info = []\n",
    "    for i, data in enumerate(loader_train):\n",
    "        counter +=1\n",
    "        x_batch,y_batch = data\n",
    "        info.append({})\n",
    "        for i in range(len(x_batch)):\n",
    "            x = x_batch[i]\n",
    "            y = y_batch[i]\n",
    "            info[counter-1][\"x\"] = x\n",
    "            info[counter-1][\"y\"] = y\n",
    "            optimizer.zero_grad()\n",
    "            output1, output2, output3 = net(x.float())\n",
    "            loss1 = criterion(output1.reshape(1).float(), y[[0]].float())\n",
    "            loss2 = criterion(output2.reshape(1).float(), y[[1]].float())\n",
    "            loss3 = criterion(output3.reshape(1).float(), y[[2]].float())\n",
    "            loss = (loss1*10)+(loss2*10)+(loss3*10)\n",
    "            info[counter-1][\"output\"] = [output1.item(),output2.item(),output3.item()]\n",
    "            info[counter-1][\"loss\"] = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(x[0])\n",
    "            #print(output1.item(), output2.item(), output3.item())\n",
    "            #print(y[0].item(), y[1].item(), y[2].item())\n",
    "            #print(loss.item())\n",
    "            #print()\n",
    "            #if(loss.item()>=0.20):\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if counter%10000 == 0:\n",
    "           # print(losses[-1])\n",
    "            print(output1.item(), output2.item(), output3.item())\n",
    "            print(y[0].item(), y[1].item(), y[2].item())\n",
    "            print(loss.item())\n",
    "            print()\n",
    "    print(f'avg train loss: {sum(losses)/len(losses)}')\n",
    "    return losses,eval_loss, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e534d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "train_losses = []\n",
    "infos_train = []\n",
    "infos_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21c51601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/300 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input values: tensor([[ 3.6117e+00,  1.7368e+00,  0.0000e+00, -5.8388e-02,  8.9145e-05,\n",
      "          2.5293e-01,  9.8683e-01,  1.2620e-01,  2.7105e+00,  3.1783e+00],\n",
      "        [ 3.9521e+00,  1.6311e+00, -1.0477e-02, -6.8865e-02,  9.5746e-05,\n",
      "          2.6421e-01,  9.6327e-01,  1.6927e-01,  3.0509e+00,  3.5187e+00],\n",
      "        [ 3.2205e+00,  1.7368e+00,  8.9690e-03, -4.9419e-02,  8.9146e-05,\n",
      "          2.4028e-01,  9.9870e-01,  1.1552e-01,  2.3193e+00,  2.7871e+00],\n",
      "        [ 3.5705e+00,  2.0671e+00,  1.2356e-02, -4.6032e-02,  6.8669e-05,\n",
      "          2.5769e-01,  1.0080e+00,  7.3164e-02,  2.6693e+00,  3.1371e+00],\n",
      "        [ 3.6084e+00,  1.5941e+00, -4.9450e-03, -6.3333e-02,  9.8053e-05,\n",
      "          2.4962e-01,  9.7121e-01,  1.6976e-01,  2.7072e+00,  3.1750e+00],\n",
      "        [ 3.3752e+00,  2.0404e+00,  1.4825e-02, -4.3563e-02,  7.0314e-05,\n",
      "          2.5124e-01,  1.0136e+00,  6.9353e-02,  2.4740e+00,  2.9418e+00],\n",
      "        [ 3.7080e+00,  1.6671e+00, -4.5090e-03, -6.2897e-02,  9.3492e-05,\n",
      "          2.5503e-01,  9.7648e-01,  1.4855e-01,  2.8068e+00,  3.2746e+00],\n",
      "        [ 3.4217e+00,  1.9566e+00,  1.1248e-02, -4.7140e-02,  7.5494e-05,\n",
      "          2.5108e-01,  1.0085e+00,  7.9775e-02,  2.5205e+00,  2.9883e+00],\n",
      "        [ 3.6647e+00,  1.9274e+00,  5.7950e-03, -5.2593e-02,  7.7305e-05,\n",
      "          2.5846e-01,  9.9888e-01,  9.1134e-02,  2.7636e+00,  3.2314e+00],\n",
      "        [ 3.6557e+00,  1.5179e+00, -8.7710e-03, -6.7159e-02,  1.0165e-04,\n",
      "          2.4963e-01,  9.5874e-01,  2.0433e-01,  2.7545e+00,  3.2223e+00],\n",
      "        [ 3.6132e+00,  1.8033e+00,  2.3150e-03, -5.6073e-02,  8.5006e-05,\n",
      "          2.5435e-01,  9.9245e-01,  1.1125e-01,  2.7120e+00,  3.1798e+00],\n",
      "        [ 3.7153e+00,  2.0132e+00,  8.0970e-03, -5.0291e-02,  7.2000e-05,\n",
      "          2.6161e-01,  1.0011e+00,  8.1830e-02,  2.8141e+00,  3.2819e+00],\n",
      "        [ 3.5585e+00,  1.5564e+00, -5.0460e-03, -6.3434e-02,  1.0042e-04,\n",
      "          2.4683e-01,  9.6771e-01,  1.8346e-01,  2.6573e+00,  3.1251e+00],\n",
      "        [ 3.3173e+00,  1.7024e+00,  5.6030e-03, -5.2785e-02,  9.1292e-05,\n",
      "          2.4229e-01,  9.9267e-01,  1.2657e-01,  2.4161e+00,  2.8839e+00],\n",
      "        [ 3.4666e+00,  1.8033e+00,  5.3300e-03, -5.3058e-02,  8.5006e-05,\n",
      "          2.4937e-01,  9.9728e-01,  1.0667e-01,  2.5654e+00,  3.0332e+00],\n",
      "        [ 3.7583e+00,  1.7368e+00, -2.9080e-03, -6.1296e-02,  8.9148e-05,\n",
      "          2.5836e-01,  9.8167e-01,  1.3130e-01,  2.8572e+00,  3.3250e+00],\n",
      "        [ 3.8586e+00,  1.8668e+00,  3.3300e-04, -5.8055e-02,  8.1062e-05,\n",
      "          2.6441e-01,  9.8818e-01,  1.0720e-01,  2.9574e+00,  3.4252e+00],\n",
      "        [ 3.5067e+00,  1.4416e+00, -7.4890e-03, -6.5877e-02,  9.8286e-05,\n",
      "          2.4187e-01,  9.5031e-01,  2.4663e-01,  2.6055e+00,  3.0733e+00],\n",
      "        [ 3.5192e+00,  1.9566e+00,  9.4220e-03, -4.8966e-02,  7.5494e-05,\n",
      "          2.5414e-01,  1.0054e+00,  8.2600e-02,  2.6180e+00,  3.0858e+00],\n",
      "        [ 3.2233e+00,  1.8354e+00,  1.1672e-02, -4.6716e-02,  8.3011e-05,\n",
      "          2.4269e-01,  1.0069e+00,  9.3996e-02,  2.3221e+00,  2.7899e+00],\n",
      "        [ 3.2764e+00,  1.9852e+00,  1.4960e-02, -4.3428e-02,  7.3726e-05,\n",
      "          2.4734e-01,  1.0143e+00,  7.2018e-02,  2.3752e+00,  2.8430e+00],\n",
      "        [ 3.9031e+00,  1.6311e+00, -9.6360e-03, -6.8024e-02,  9.5745e-05,\n",
      "          2.6216e-01,  9.6517e-01,  1.6735e-01,  3.0019e+00,  3.4697e+00],\n",
      "        [ 3.5155e+00,  1.8033e+00,  4.2990e-03, -5.4089e-02,  8.5007e-05,\n",
      "          2.5099e-01,  9.9571e-01,  1.0814e-01,  2.6143e+00,  3.0821e+00],\n",
      "        [ 3.2215e+00,  1.7704e+00,  9.8830e-03, -4.8505e-02,  8.7051e-05,\n",
      "          2.4112e-01,  1.0017e+00,  1.0751e-01,  2.3203e+00,  2.7881e+00],\n",
      "        [ 3.9533e+00,  1.7024e+00, -7.6290e-03, -6.6017e-02,  9.1293e-05,\n",
      "          2.6546e-01,  9.7101e-01,  1.4757e-01,  3.0521e+00,  3.5199e+00],\n",
      "        [ 3.4674e+00,  1.8354e+00,  6.3690e-03, -5.2019e-02,  8.3011e-05,\n",
      "          2.5008e-01,  9.9968e-01,  1.0039e-01,  2.5662e+00,  3.0340e+00],\n",
      "        [ 3.6532e+00,  1.4047e+00, -1.2724e-02, -7.1112e-02,  9.6079e-05,\n",
      "          2.4670e-01,  9.3817e-01,  2.7971e-01,  2.7520e+00,  3.2198e+00],\n",
      "        [ 3.3644e+00,  1.6311e+00,  2.2860e-03, -5.6102e-02,  9.5745e-05,\n",
      "          2.4198e-01,  9.8343e-01,  1.4949e-01,  2.4632e+00,  2.9310e+00],\n",
      "        [ 4.0045e+00,  1.8354e+00, -3.0290e-03, -6.1417e-02,  8.3011e-05,\n",
      "          2.6953e-01,  9.8040e-01,  1.1803e-01,  3.1033e+00,  3.5711e+00],\n",
      "        [ 3.3722e+00,  1.9274e+00,  1.1276e-02, -4.7112e-02,  7.7306e-05,\n",
      "          2.4899e-01,  1.0085e+00,  8.2385e-02,  2.4710e+00,  2.9388e+00],\n",
      "        [ 3.5693e+00,  2.0132e+00,  1.0487e-02, -4.7901e-02,  7.2000e-05,\n",
      "          2.5676e-01,  1.0062e+00,  7.7566e-02,  2.6681e+00,  3.1359e+00],\n",
      "        [ 3.5651e+00,  1.8354e+00,  4.3870e-03, -5.4001e-02,  8.3011e-05,\n",
      "          2.5333e-01,  9.9649e-01,  1.0337e-01,  2.6639e+00,  3.1317e+00],\n",
      "        [ 3.8143e+00,  2.0932e+00,  9.6520e-03, -4.8736e-02,  6.7063e-05,\n",
      "          2.6613e-01,  9.9994e-01,  7.7927e-02,  2.9131e+00,  3.3809e+00],\n",
      "        [ 3.8127e+00,  2.0132e+00,  6.6310e-03, -5.1757e-02,  7.2000e-05,\n",
      "          2.6497e-01,  9.9751e-01,  8.4536e-02,  2.9115e+00,  3.3793e+00],\n",
      "        [ 3.8548e+00,  1.6671e+00, -7.3540e-03, -6.5742e-02,  9.3491e-05,\n",
      "          2.6083e-01,  9.7111e-01,  1.5399e-01,  2.9536e+00,  3.4214e+00],\n",
      "        [ 3.6109e+00,  1.7024e+00, -1.1960e-03, -5.9584e-02,  9.1292e-05,\n",
      "          2.5216e-01,  9.8353e-01,  1.3509e-01,  2.7097e+00,  3.1775e+00],\n",
      "        [ 3.8541e+00,  1.6311e+00, -8.7480e-03, -6.7136e-02,  9.5744e-05,\n",
      "          2.6013e-01,  9.6703e-01,  1.6546e-01,  2.9529e+00,  3.4207e+00],\n",
      "        [ 3.7165e+00,  2.0671e+00,  1.0072e-02, -4.8316e-02,  6.8669e-05,\n",
      "          2.6246e-01,  1.0029e+00,  7.7099e-02,  2.8153e+00,  3.2831e+00],\n",
      "        [ 3.4253e+00,  2.0932e+00,  1.5668e-02, -4.2720e-02,  6.7063e-05,\n",
      "          2.5363e-01,  1.0136e+00,  6.7584e-02,  2.5241e+00,  2.9919e+00],\n",
      "        [ 3.7515e+00,  1.4047e+00, -1.5210e-02, -7.3598e-02,  9.6083e-05,\n",
      "          2.5089e-01,  9.3483e-01,  2.8240e-01,  2.8503e+00,  3.3181e+00],\n",
      "        [ 3.6541e+00,  1.4416e+00, -1.1444e-02, -6.9832e-02,  9.8283e-05,\n",
      "          2.4768e-01,  9.4561e-01,  2.5028e-01,  2.7528e+00,  3.2206e+00],\n",
      "        [ 3.4705e+00,  1.9566e+00,  1.0325e-02, -4.8063e-02,  7.5494e-05,\n",
      "          2.5259e-01,  1.0070e+00,  8.1167e-02,  2.5693e+00,  3.0371e+00],\n",
      "        [ 4.0028e+00,  1.7368e+00, -6.9910e-03, -6.5379e-02,  8.9147e-05,\n",
      "          2.6803e-01,  9.7239e-01,  1.4031e-01,  3.1016e+00,  3.5694e+00],\n",
      "        [ 3.7170e+00,  2.0932e+00,  1.1027e-02, -4.7361e-02,  6.7063e-05,\n",
      "          2.6285e-01,  1.0035e+00,  7.5550e-02,  2.8158e+00,  3.2836e+00],\n",
      "        [ 3.4161e+00,  1.7368e+00,  4.3060e-03, -5.4082e-02,  8.9146e-05,\n",
      "          2.4623e-01,  9.9313e-01,  1.2016e-01,  2.5149e+00,  2.9827e+00],\n",
      "        [ 3.8079e+00,  1.7704e+00, -2.5330e-03, -6.0921e-02,  8.7052e-05,\n",
      "          2.6087e-01,  9.8284e-01,  1.2506e-01,  2.9067e+00,  3.3745e+00],\n",
      "        [ 3.5104e+00,  1.5941e+00, -2.5810e-03, -6.0969e-02,  9.8054e-05,\n",
      "          2.4601e-01,  9.7440e-01,  1.6671e-01,  2.6092e+00,  3.0770e+00],\n",
      "        [ 3.7508e+00,  1.3686e+00, -1.6520e-02, -7.4908e-02,  9.3927e-05,\n",
      "          2.4998e-01,  9.2674e-01,  3.1730e-01,  2.8496e+00,  3.3174e+00],\n",
      "        [ 3.8498e+00,  1.4047e+00, -1.7489e-02, -7.5877e-02,  9.6080e-05,\n",
      "          2.5527e-01,  9.3131e-01,  2.8541e-01,  2.9486e+00,  3.4164e+00],\n",
      "        [ 3.4239e+00,  2.0404e+00,  1.3951e-02, -4.4437e-02,  7.0314e-05,\n",
      "          2.5269e-01,  1.0120e+00,  7.0848e-02,  2.5227e+00,  2.9905e+00]])\n",
      " shape: torch.Size([50, 10])\n",
      "embedding output: tensor([[-1.6812e-01,  5.1966e-04,  9.6577e-05,  ...,  1.0945e-01,\n",
      "         -1.1205e-02, -6.8399e-02],\n",
      "        [-1.6880e-01,  1.0504e-03, -9.1654e-04,  ...,  1.0921e-01,\n",
      "         -1.1883e-02, -6.8233e-02],\n",
      "        [-1.6750e-01,  2.1749e-04,  9.6422e-04,  ...,  1.0983e-01,\n",
      "         -1.0439e-02, -6.8576e-02],\n",
      "        ...,\n",
      "        [-1.6945e-01,  1.7478e-03, -1.9202e-03,  ...,  1.0930e-01,\n",
      "         -1.2327e-02, -6.7666e-02],\n",
      "        [-1.6939e-01,  1.6734e-03, -1.8151e-03,  ...,  1.0927e-01,\n",
      "         -1.2333e-02, -6.7826e-02],\n",
      "        [-1.6717e-01, -7.1889e-04,  1.5060e-03,  ...,  1.0939e-01,\n",
      "         -1.0461e-02, -6.8065e-02]])\n",
      " shape: torch.Size([50, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weighted_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m300\u001b[39m)):\n\u001b[1;32m----> 2\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mextend(test())\n\u001b[0;32m      3\u001b[0m     train_lossTemp,val_lossTemp, info \u001b[38;5;241m=\u001b[39m train_one_epoch()\n\u001b[0;32m      4\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mextend(train_lossTemp)\n",
      "Cell \u001b[1;32mIn [19], line 12\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m x_batch[i]\n\u001b[0;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m y_batch[i]\n\u001b[1;32m---> 12\u001b[0m output1, output2, output3 \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m criterion(output1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), y[[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     14\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m criterion(output2\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), y[[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rans\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [13], line 20\u001b[0m, in \u001b[0;36mMainNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#weighted_output = self.weightedSum(embedding_output, x)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweighted_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweighted_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m weighted_output_flattened \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(embedding_output)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print(f'weighted flattened: {weighted_output_flattened}\\n shape: {weighted_output_flattened.shape}')\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weighted_output' is not defined"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(300)):\n",
    "    val_losses.extend(test())\n",
    "    train_lossTemp,val_lossTemp, info = train_one_epoch()\n",
    "    train_losses.extend(train_lossTemp)\n",
    "    infos_train.extend(info)\n",
    "    #infos_val.extend(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10223383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#for i, el in enumerate(infos_train):\n",
    "    #x = dataset.original_x[i][-2]\n",
    "    #y = dataset.original_x[i][-1]\n",
    "    #print(x, y)\n",
    "    #out = el[\"output\"][2]\n",
    "    #y = el[\"y\"][2].item()\n",
    "    #print(pos)\n",
    "    #ax.scatter(x = x, y = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb64eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageOut(list_nums, n):\n",
    "    new_list = []\n",
    "    temp_sum = 0\n",
    "    temp_length = 0\n",
    "    counter = 0\n",
    "    average = 0\n",
    "    temp_list = []\n",
    "    final_list = []\n",
    "    for i, num in enumerate(list_nums):\n",
    "        counter+=1\n",
    "        temp_list.append(num)\n",
    "        if counter == n:\n",
    "            avg = sum(temp_list) / len(temp_list)\n",
    "            final_list.append(avg)\n",
    "            temp_list = []\n",
    "            counter = 0\n",
    "    if len(temp_list) != 0:\n",
    "        avg = sum(temp_list) / len(temp_list)\n",
    "        final_list.append(avg)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfe54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.plot(averageOut(train_losses, 1000), color=\"red\", label=\"training loss\")\n",
    "plt.plot(averageOut(val_losses, 1000), color=\"blue\", label=\"validation loss\")\n",
    "plt.title(\"validation & training loss (log scale)\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./images/loss_4.png\", dpi=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./models/net_C=1000_S=50_E=300.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"./models/net.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "287199f5fcea6f9a85665f99d67797111be3aee227282d66a776eae29d9b3874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
